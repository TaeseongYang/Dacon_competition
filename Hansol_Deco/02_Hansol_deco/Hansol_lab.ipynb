{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNGkJfKKfkC2XvTM2XMQldk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7lNSs8TJ4qY","executionInfo":{"status":"ok","timestamp":1709715835684,"user_tz":-540,"elapsed":5250,"user":{"displayName":"양태성","userId":"11727281793413751368"}},"outputId":"2d62e0f7-840d-47cf-a137-3e4ecdb721a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","from tqdm.auto import tqdm\n","import random\n","import os\n","\n","def reset_seeds(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","DATA_PATH = \"/content/drive/MyDrive/Hanso_deco/data/\"\n","SEED = 42\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"-YqufMnoKSZI","executionInfo":{"status":"ok","timestamp":1709715835684,"user_tz":-540,"elapsed":11,"user":{"displayName":"양태성","userId":"11727281793413751368"}},"outputId":"471aa67b-71cb-482a-81c8-1302ac036bf5"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["train_ft = pd.read_csv(f'{DATA_PATH}train_data.csv')\n","train_ft.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kI6LibEKmlX","executionInfo":{"status":"ok","timestamp":1709715835685,"user_tz":-540,"elapsed":11,"user":{"displayName":"양태성","userId":"11727281793413751368"}},"outputId":"035dccb4-210e-4d5e-e2d1-7c7b50db8563"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6440, 2)"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["test_ft = pd.read_csv(f'{DATA_PATH}test.csv')\n","test_ft.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8J7Cr0TKqrK","executionInfo":{"status":"ok","timestamp":1709715835685,"user_tz":-540,"elapsed":9,"user":{"displayName":"양태성","userId":"11727281793413751368"}},"outputId":"8d7659f4-ef96-407f-b396-8ce218fe6bc7"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(130, 2)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM"],"metadata":{"id":"xN65UnFGKyH8","executionInfo":{"status":"ok","timestamp":1709715835685,"user_tz":-540,"elapsed":7,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# model_name = 'beomi/llama-2-ko-7b'\n","model_name = 'skt/kogpt2-base-v2'\n","model = AutoModelForCausalLM.from_pretrained(model_name)"],"metadata":{"id":"j2saBGsCKyg-","executionInfo":{"status":"ok","timestamp":1709715836925,"user_tz":-540,"elapsed":1247,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name,\n","                              bos_token='</s>',  # start 토큰\n","                              eos_token='</s>',  # end 토큰\n","                              unk_token='<unk>',\n","                              pad_token='<pad>',\n","                              mask_token='<mask>',\n","                              max_len = 512)"],"metadata":{"id":"PvcqaXLkpsyF","executionInfo":{"status":"ok","timestamp":1709715838352,"user_tz":-540,"elapsed":1430,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["class ChatDataset(torch.utils.data.Dataset):\n","    def __init__(self,df):\n","        self.question = df[\"질문\"].tolist()\n","        self.answer = df[\"답변\"].tolist()\n","\n","    def __len__(self):\n","        return len(self.question)\n","    def __getitem__(self, idx):\n","        return \"<q>\" + self.question[idx] + \"</s><a>\" + self.answer[idx] + \"</s>\""],"metadata":{"id":"2Rt_eksNpv5g","executionInfo":{"status":"ok","timestamp":1709715838353,"user_tz":-540,"elapsed":8,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["class CollateFN:\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","    def __call__(self, batch):\n","        x = self.tokenizer(batch, return_tensors=\"pt\",padding=True)\n","        return {\"x\" : x}"],"metadata":{"id":"Q4EWXPjNpzmg","executionInfo":{"status":"ok","timestamp":1709715838353,"user_tz":-540,"elapsed":8,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["collate_fn = CollateFN(tokenizer)"],"metadata":{"id":"loivXTHjp1NA","executionInfo":{"status":"ok","timestamp":1709715838353,"user_tz":-540,"elapsed":7,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["dt = ChatDataset(train_ft)\n","dl = torch.utils.data.DataLoader(dt, batch_size=2, shuffle=True, collate_fn=collate_fn)\n","batch = next(iter(dl))\n","batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8criVcBp2Sf","executionInfo":{"status":"ok","timestamp":1709715838353,"user_tz":-540,"elapsed":7,"user":{"displayName":"양태성","userId":"11727281793413751368"}},"outputId":"3d30d8f2-b708-4e44-b4e0-5287f478e317"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'x': {'input_ids': tensor([[ 9724,   455,   405,  8168,  7433,  8137, 43519,  9068, 11200,  8705,\n","          15333, 24488,  8084,   406,     1,  9724,   439,   405,  8168,  7433,\n","           8137, 10246, 17947, 11357,  9774,  9340, 11200,  8705, 12425, 32987,\n","          10635,   387,  9110,  7433, 15813, 19449, 14579, 10635, 11343, 16977,\n","          28112,  9855,  9133, 30470,  9199,  9677, 16691,  9394, 43519, 10679,\n","           9306, 10829, 12972, 29247, 37428,  9746, 10247,  6824, 11039, 12987,\n","          30470,  9199, 11350, 37194,  9625, 10679, 19466, 33204,  9359,  7991,\n","          11584, 42229,  6859,  9185,  9025,  6855,  9915, 15309, 10146, 18566,\n","          10599, 10586,  9597, 21154, 10783, 46394, 14653,  9185,  9110,  7433,\n","           8143, 41653, 27117, 14611, 13784, 41248, 49421,     1,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3],\n","         [ 9724,   455,   405, 26600, 11037,  9836,  9082,  8467, 23917, 25975,\n","          11649,  9050,  7161,  8704, 20456, 18422,  9695,  9025,  9846,  6969,\n","           8084,   406,     1,  9724,   439,   405, 15953,  9082,  8467, 23917,\n","          25975,  9050,  7161,  8704, 20456, 18422, 15381, 11357,  9774,  9340,\n","          24804, 36375,  9025, 32987,  9558,  9207,  9703,  8146,  7889,  9028,\n","          12744, 27034, 10607, 14921, 11314,  9554, 20152, 30770, 10599, 10586,\n","           9597, 21154, 50373, 10587,  9703,  8146,  7889, 38652,  7691,  6903,\n","           9066,  9775, 10856,  9185, 10336, 10676, 12128,  8705,  9025,  9080,\n","          27034, 18422, 30770,  9199,  9677, 16691,  9322,  9836, 11153, 12589,\n","           9538,  7374,  7182, 12119,  9021, 14253,  7252, 10336,  8091, 45165,\n","           9185, 16376, 10623, 38289, 13266, 18735, 16278,  8705,  9788, 32987,\n","          10221, 27134,  9185,  9141,  6905, 10336,  8091, 10336,  9693, 13266,\n","           9065,  9372, 13163, 10234,  9050,  7161, 10726, 16278,  8705,  9788,\n","          32987,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}}"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer, device):\n","    epoch_loss = 0\n","    model.train()\n","    for batch in tqdm(dataloader):\n","        x = batch[\"x\"].to(device)\n","        pred = model(**x).logits # 예측값 batch, seq, n_class\n","        n_class = pred.shape[-1] # 정답 클래스 개수\n","        pred = pred[:,:-1].reshape(-1, n_class) # batch x seq, n_class\n","\n","        trg = x[\"input_ids\"][:,1:].flatten() # batch x seq\n","\n","        mask = trg != 3\n","        trg = trg[mask]\n","        pred = pred[mask]\n","\n","        loss = loss_fn(pred, trg)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    epoch_loss /= len(dataloader)\n","\n","    return epoch_loss"],"metadata":{"id":"obicEy92p4OJ","executionInfo":{"status":"ok","timestamp":1709715838353,"user_tz":-540,"elapsed":6,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["batch_size = 14\n","loss_fn = torch.nn.CrossEntropyLoss()\n","epochs = 15"],"metadata":{"id":"uWXP6x3EqWzg","executionInfo":{"status":"ok","timestamp":1709715838353,"user_tz":-540,"elapsed":6,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["reset_seeds(SEED)\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n","\n","train_dt = ChatDataset(train_ft)\n","train_dl = torch.utils.data.DataLoader(train_dt, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","\n","for i in range(epochs):\n","    loss = train_loop(train_dl, model, loss_fn, optimizer, device)\n","    print(i, \"번째: \", loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"id":"PjuwzrC7qaAv","executionInfo":{"status":"error","timestamp":1709715839476,"user_tz":-540,"elapsed":1129,"user":{"displayName":"양태성","userId":"11727281793413751368"}},"outputId":"f9bf8cde-4817-4737-b34a-f73539026e2a"},"execution_count":56,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 137.06 MiB is free. Process 8041 has 14.61 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 477.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-dada3d264bb5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreset_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m                 )\n\u001b[0;32m-> 2556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1157\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 137.06 MiB is free. Process 8041 has 14.61 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 477.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["def chatbot(model, tokenizer, max_len, device, test_ft):\n","    model.eval()\n","    results = []\n","    for text in '<q>' + test_ft['질문'] + '</s><a>':\n","        x = tokenizer.encode(text, return_tensors='pt').to(device)\n","        result = model.generate(x,\n","                                max_length=max_len,\n","                                use_cache=True,\n","                                repetition_penalty=4.8,\n","                                temperature = 0.1,\n","                                top_k = 40,\n","                                )\n","        q_len = len(text) + 1\n","        result = tokenizer.decode(result[0])\n","        results.append(result[q_len:-4])\n","\n","    test_ft['답변'] = results\n","    return test_ft"],"metadata":{"id":"huwJELbB1WVc","executionInfo":{"status":"ok","timestamp":1709713126094,"user_tz":-540,"elapsed":638,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["result_df = chatbot(model, tokenizer, 256, device, test_ft)\n","result_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hls1vJGF1W_V","executionInfo":{"status":"ok","timestamp":1709714296155,"user_tz":-540,"elapsed":325451,"user":{"displayName":"양태성","userId":"11727281793413751368"}},"outputId":"282883bb-f132-4e9a-889d-13cbe8d38731"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["(130, 3)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["result_df.to_csv('result_df.csv',index = False)"],"metadata":{"id":"J61wPM-t1Ycz","executionInfo":{"status":"ok","timestamp":1709714538338,"user_tz":-540,"elapsed":6,"user":{"displayName":"양태성","userId":"11727281793413751368"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"w1rJL-bS6xVo"},"execution_count":null,"outputs":[]}]}